<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>replication | Sam Harper</title>
    <link>http://sbh4th.github.io/tag/replication/</link>
      <atom:link href="http://sbh4th.github.io/tag/replication/index.xml" rel="self" type="application/rss+xml" />
    <description>replication</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Â© 2021 Sam Harper</copyright><lastBuildDate>Tue, 15 Oct 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>http://sbh4th.github.io/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_3.png</url>
      <title>replication</title>
      <link>http://sbh4th.github.io/tag/replication/</link>
    </image>
    
    <item>
      <title>Clusterjerk: Peer Review Edition!</title>
      <link>http://sbh4th.github.io/post/peer-review/</link>
      <pubDate>Tue, 15 Oct 2019 00:00:00 +0000</pubDate>
      <guid>http://sbh4th.github.io/post/peer-review/</guid>
      <description>&lt;h2 id=&#34;i&#34;&gt;I.&lt;/h2&gt;
&lt;p&gt;This week I saw an interesting &lt;a href=&#34;http://www.nber.org/papers/w26364&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper&lt;/a&gt; published by NBER by D. Mark Anderson, Kyutaro Matsuzawa, and Joseph J. Sabia (an ungated version is posted on Anderson&amp;rsquo;s &lt;a href=&#34;http://www.dmarkanderson.com/AMS_Marriage_Equality_Teen_Suicide_Attempts_October_3_2019v3.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;website&lt;/a&gt;) about the impact of marriage equality laws on youth suicide risk.&lt;/p&gt;
&lt;p&gt;Obviously, this paper is interesting on it&amp;rsquo;s own: Marriage equality laws may have plausible impacts on mental health, especially among marginalized populations. Anderson et al. also argue that there could be some potential negative consequences for mental health due to political backlash. Thus designing a study to evaluate the impact of legislation on youth suicide piqued my interest.&lt;/p&gt;
&lt;p&gt;As it turns out, it seems that Anderson et al. also had &lt;em&gt;their&lt;/em&gt; interest piqued by a previous paper on this topic, which was published in &lt;em&gt;JAMA Pediatrics&lt;/em&gt;. They start by noting:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A recent article published by Raifman et al. (2017) produces the first empirical evidence on the relationship between SSM legalization and youth mental health. Using data from the State Youth Risk Behavior Surveys (YRBS) for the period 1999-2015, Raifman et al. (2017) find that legalization is associated with a 0.6 percentage-point (7 percent) decline in self-reported suicide attempts among all high school students, and a 4 percentage-point (14 percent) decline in suicide attempts among those who identified as LGBQ relative to suicide attempts among heterosexual-identifying youth.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Hey&amp;hellip;wait a minute&amp;hellip;something seems familiar here&amp;hellip;Oh yeah! I know that paper! In fact, I &lt;em&gt;reviewed&lt;/em&gt; the Raifman (2017) paper for &lt;em&gt;JAMA Pediatrics&lt;/em&gt; a couple of years ago:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /post/peer-review/jp-email_hu7ea811762075c66be5ec0e8bc7955154_42891_70835d3de2dea054ec74373e8eb1d6d4.png 400w,
               /post/peer-review/jp-email_hu7ea811762075c66be5ec0e8bc7955154_42891_7577931a30ec2b0a7ef5afeac8e666e7.png 760w,
               /post/peer-review/jp-email_hu7ea811762075c66be5ec0e8bc7955154_42891_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;http://sbh4th.github.io/post/peer-review/jp-email_hu7ea811762075c66be5ec0e8bc7955154_42891_70835d3de2dea054ec74373e8eb1d6d4.png&#34;
               width=&#34;760&#34;
               height=&#34;120&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Well. This is indeed interesting. Let&amp;rsquo;s see where it goes&amp;hellip;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;While there is much to admire about the pioneering efforts of Raifman et al. (2017),&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Alright, I&amp;rsquo;m with you&amp;hellip;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;we believe their conclusions deserve closer scrutiny for a number of reasons&amp;hellip;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Ohhhh, now I think I see what&amp;rsquo;s going on. This feels like replication-town (I&amp;rsquo;ve been there &lt;a href=&#34;https://sbh4th.github.io/post/420-update/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;before&lt;/a&gt;. It can be nice, but stormy). They make a few arguments regarding limitations of the Raifman paper (which I think can be read for free &lt;a href=&#34;https://jamanetwork.com/journals/jamapediatrics/fullarticle/2604258&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;), summarized below.&lt;/p&gt;
&lt;p&gt;What are their reasons for re-evaluating this question? They give several:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Access to additional post-treatment data;&lt;/li&gt;
&lt;li&gt;More flexible difference-in-difference-in-differences (yes, that) specification;&lt;/li&gt;
&lt;li&gt;Different level of clustering for standard errors;&lt;/li&gt;
&lt;li&gt;Rescaled survey weights;&lt;/li&gt;
&lt;li&gt;Composisitional impacts;&lt;/li&gt;
&lt;li&gt;More outcomes;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I won&amp;rsquo;t discuss all of these, but I certainly encourage you to read the entire paper and judge for yourself. Here is Anderson et al.&amp;rsquo;s bottom line:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Despite previous research suggesting otherwise, we find little evidence that SSM laws have reduced suicide attempts among teen sexual minorities, nor have they decreased the likelihood of suicide planning, suicide ideation, or depression. Instead, we find some evidence that SSM legalization via judicial mandate is associated with worse mental health for these individuals, consistent with
a story of social backlash.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;ii&#34;&gt;II.&lt;/h2&gt;
&lt;p&gt;Next, they try and replicate several of the findings of Raifman et al. (2017). I won&amp;rsquo;t digress too much here, except to say that it may come as little surprise to anyone that &lt;em&gt;JAMA Pediatrics&lt;/em&gt; does not require (or ask) authors to post either datasets or code that would help to facilitate straightforward computational replication. However, the data are public and the design is relatively straightforward, and Raifman et al. were pretty clear about what they did.&lt;/p&gt;
&lt;p&gt;Here is what Anderson et al. found when they tried to replicate the main findings:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In column (1) of Table 3, we attempt to replicate the original findings of Raifman et al. (2017). Following Raifman et al. (2017), we estimate equation (1), adjusting standard errors for clustering at the state-by-grade level and weighting regressions using the State YRBS-provided sampling weights. As discussed above, clustering at the state-by-grade level may lead to standard errors that are too small and the State YRBS weights are not designed to be comparable across states or even within states over time. Based on this specification, we find that SSM laws are associated with a 0.66 percentage point decrease in suicide attempts among U.S. high school students. This estimate is statistically distinguishable from zero at the 5 percent level and is nearly identical to the estimate reported in Raifman et al. (2017).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;em&gt;Nearly identical&lt;/em&gt; estimates seems pretty great, and of course this really helps Anderson et al. make their case for how other kinds of choices may have impacted the Raifman et al. results.&lt;/p&gt;
&lt;p&gt;Now here is the part that got me going:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In column (2), we correct the standard errors by adjusting them for clustering them at the level of policy variation (i.e., the state). This adjustment results in a 56 percent increase in the estimated standard error, rendering the estimate statistically indistinguishable from zero at conventional levels.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So, they find that the standard errors are sensitive to whether they are clustered at the state vs. the state-grade level. In terms of point estimates and 95% confidence intervals, here are those estimates from Anderson et al.:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Level of clustering&lt;/th&gt;
&lt;th&gt;Estimate (percentage points)&lt;/th&gt;
&lt;th&gt;95%CI&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;State, grade&lt;/td&gt;
&lt;td&gt;-0.66&lt;/td&gt;
&lt;td&gt;-1.2, -0.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;State&lt;/td&gt;
&lt;td&gt;-0.66&lt;/td&gt;
&lt;td&gt;-1.5, 0.2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Now, there is, of course, a larger discussion to be had about how we should feel about the difference between those two estimates. Anderson et al. focus on the new results being &amp;ldquo;statistically indistiguishingable&amp;rdquo; from the zero. If one wanted to, one could obtain the analogous &lt;a href=&#34;https://www.bmj.com/content/343/bmj.d2304&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;p-values&lt;/a&gt; for these estimates, and I&amp;rsquo;m sure readers won&amp;rsquo;t be surprised where that would lead. But the larger point is that dealing with clustering for group-level treatments is important and there has been a lot of discussion about it (the title of this post is a riff on a &lt;a href=&#34;https://chrisblattman.com/2015/12/08/clusterjerk/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;series&lt;/a&gt; of &lt;a href=&#34;https://chrisblattman.com/2015/12/11/clusterjerk-the-much-anticipated-sequel/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;blog&lt;/a&gt; &lt;a href=&#34;http://john-joseph-horton.com/monte-carlo-clusterjerk/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;posts&lt;/a&gt; by the economist Chris Blattman and others discussing the issue of clustered standard errors).&lt;/p&gt;
&lt;p&gt;The other issue I&amp;rsquo;ll discuss is heterogenous effects by sexual minority status. Raifman et al. (2017) reported that the impact of laws in reducing suicide attempts was stronger among sexual minorities (their estimate was -4.0 with a 95%CI of -6.9 to -1.2). However, as Anderson et al. point out, their model only forces all covariates (including state and year fixed effects) to be identical for sexual minorities and heterosexuals. They allow these to differ by using a fully interacted model and compare:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;Estimate&lt;/th&gt;
&lt;th&gt;95%CI&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Sexual minorities (partially interacted)&lt;/td&gt;
&lt;td&gt;-4.2&lt;/td&gt;
&lt;td&gt;-6.7, -1.6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Sexual minorities (fully interacted)&lt;/td&gt;
&lt;td&gt;-1.6&lt;/td&gt;
&lt;td&gt;-4.9, 1.7&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Again, the first estimates are nearly identical to Raifman et al. (2017), but this model choice has serious consequences and reduces the impact among this subpopulation, as Anderson et al. note:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In panel II of Table 4, we restrict the sample to sexual minorities, allowing the effects of the covariates to differ for this subgroup. In this case, the absolute magnitude of the estimated policy impact falls by 67 percent; we consistently find that SSM laws are associated with a statistically insignificant 1.4 to 1.5 percentage-point reduction in suicide attempts. In panel III, we find no evidence that SSM laws affect suicide attempts among self-identified heterosexuals, and estimates from fully-interacted DDD models (panel IV) are in line with those shown in panel II, suggesting that forcing coefficients on covariates to be equal for sexual minorities and heterosexuals may not be appropriate.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;iii&#34;&gt;III.&lt;/h2&gt;
&lt;p&gt;Okay, so why am I going on at such length about these issues? Without trying to sound petulant (and vain), it&amp;rsquo;s because I brought up both of these issues when I first reviewed the paper for &lt;em&gt;JAMA Pediatrics&lt;/em&gt; in October of 2016, yet obviously neither the authors nor the editor gave these comments much consideration.&lt;/p&gt;
&lt;p&gt;At the risk of revealing parts of my review and, in retrospect, things I wish I might have said differently (or not at all), I&amp;rsquo;ll just highlight a couple of points I made in my review. First, on the issue of covariate differences in the &amp;ldquo;partially interacted&amp;rdquo; DD model, I said:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol start=&#34;9&#34;&gt;
&lt;li&gt;I think the DDD idea is what the authors are getting at with this: âIn the analysis focused on sexual minority youth, we estimated the interaction between state same-sex marriage and sexual minority identity and included binary control indicators for state same-sex marriage and for sexual minority identity.â but this statement is not clear to me. Including the product term between minority status and the treatment variable goes further toward the DDD type estimation, but (as far as I can tell, and since they are not mentioned in Table S2) does not account for state fixed differences in the minority-majority gap in suicide attempts (which could differ by treatment status), or for the fact that there may different secular trends for the minority and majority groups that are common to all states. These can be remedied by including product terms between the state fixed effects and minority status, and the year fixed effects and minority status.&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;I did not go all the way and ask for separate models, but this is effectively the same issue regarding the DDD specification that Anderson et al. are worried about. Just including a product term in a &amp;ldquo;traditional&amp;rdquo; DD model between the DD coefficient and a group indicator makes strong assumptions about the fixed effects that may not be justified.&lt;/p&gt;
&lt;p&gt;Similarly, on the issue of clustering:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol start=&#34;11&#34;&gt;
&lt;li&gt;More importantly, do the Taylor series standard errors adjust for clustering and potential serial correlation by state? I think they do, but this is an important issue in DD analysis (see the Bertrand paper already cited). I see now in the footnote to Table 2 that SEs were clustered âschool and by classroom.â Cameron and Miller (J Hum Res 2015) discuss this situation in particular: âAt the minimum, one should cluster at the level of the primary sampling unit though often there is reason to cluster at a broader level, such as clustering on state if regressors and errors are correlated within state.â Because it seems very likely that in the YRBS data for suicide attempts errors are correlated within state and, at a minimum, the authors should also cluster standard errors at the state level in a sensitivity analysis, though my preference would be to do this for the main analysis.&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;I regret saying &amp;ldquo;I think they do,&amp;rdquo; since I should know better, but also felt like I wanted to be positive and give the authors the benefit of the doubt. Regardless, I thought I made it clear that I did not think the standard errors were clustered at the right level, gave some rationale for why they should be worried about it, and asked that they do this for the main analysis, or at least as a sensitivity analysis. Apparently they did not find this compelling.&lt;/p&gt;
&lt;p&gt;Even more frustrating, when I looked at the email from the editor giving the &amp;ldquo;Revise and Resubmit&amp;rdquo;, I can now see that another reviewer made the same comment(!):&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol start=&#34;5&#34;&gt;
&lt;li&gt;Clustering. Table 2 refers to clustering by school and classroom. But this is not described in the text. More importantly, I think the authors should show models with clustering at the state level instead of at these other levels. In general standard errors should be clustered at the level at which the treatment varies. Here, that level is state, not classroom or school. This may affect precision; results should be shown with various levels of clustering, similar to the robustness analyses currently shown.&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;So we have 2 reviewers (the only 2 reviewers, as far as I&amp;rsquo;m aware) both asking (with reasons) for the anlaysis to be done with SE clustered at the state level, yet what we end up with is a paper with SEs clustered at the &amp;ldquo;state and grade level&amp;rdquo;. Is this what we want from peer review? And for all you counterfactualists out there, what do you think &lt;em&gt;would&lt;/em&gt; have happened to this paper &lt;em&gt;at JAMA Pediatrics&lt;/em&gt; had the authors clustered the standard errors at the state level?&lt;/p&gt;
&lt;h2 id=&#34;iv&#34;&gt;IV.&lt;/h2&gt;
&lt;p&gt;The purpose of this post isn&amp;rsquo;t to comment specifically about the substantive issues under investigation in these papers (obviously, that is important, too). And it certainly isn&amp;rsquo;t to throw stones at Raifman et al. (disclosure: I know one of the authors personally). I thought the paper had many positive qualities, but needed some additional work and robustness testing. Pretty standard for a lot of DD papers.&lt;/p&gt;
&lt;p&gt;My concern is about how peer review is handled. We already have a huge problem with &lt;a href=&#34;https://www.thelancet.com/series/research&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;research waste&lt;/a&gt; in biomedical research, and having smart people like Anderson et al. spend lots of time correcting flaws in a paper that were previously identified during the &amp;ldquo;normal&amp;rdquo; process of peer review seems like a waste of time and resources. Why even bother with peer review at all? Had the first version of this paper been published without any peer review, my guess is that we would see much the same paper by Anderson et al. Yes, on one hand, I can see this as a great example of a well-designed replication paper. Anderson et al. found &amp;ldquo;much to admire&amp;rdquo; in Raifman et al. (2017), but also believed the results deserved to be further interrogated. Plus, they attempt to &lt;em&gt;advance&lt;/em&gt; the evidence further by including more post-treatment data, taking advantage of a new potential source of exogenous variation, and looking at other outcomes. All good.&lt;/p&gt;
&lt;p&gt;On the other hand, however, I&amp;rsquo;m frustrated that the process of peer review in the first go around could have been more transparent. Did the editor communicate to the authors the concerns that &lt;em&gt;both&lt;/em&gt; reviewers had about the clustering issue? Perhaps the authors argued against clustering at the state level in their response, but why then did we end up with &amp;ldquo;state-grade&amp;rdquo; clustering? When I spend my time reviewing a paper I generally hope that both the authors &lt;em&gt;and the editor&lt;/em&gt; take my comments seriously. But I don&amp;rsquo;t pretend that every comment I make in a review is equally important, or important at all. I can (and do) focus on pendantic, pet issues that bother me, but this is a case where the paper is about a serious policy (and health) issue, the authors are doing some non-standard-practice things and, as we seem to have learned from Anderson et al., these choices &lt;em&gt;actually do&lt;/em&gt; make a difference to the results and interpretation.&lt;/p&gt;
&lt;p&gt;Perhaps it is my fault for not following up more closely. In fairness, I may have ticked the box when submitting my review that I didn&amp;rsquo;t want to see the paper again, having already invested a lot of time in reviewing it and providing what I thought were helpful comments. In fact, I tried to login to the &lt;em&gt;JAMA Pediatrics&lt;/em&gt; site to retrieve my review and see how the paper was handled and perhaps whether I asked not to see it again, and not only is there no record of my review, they don&amp;rsquo;t even have me in the system as a peer reviewer!&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /post/peer-review/jp-gone_hu1e39a4b3266917a997281e8904a8c684_51636_438014fda2823f3009175cb8e99fc031.png 400w,
               /post/peer-review/jp-gone_hu1e39a4b3266917a997281e8904a8c684_51636_92b212968dd7b7c648aae309eddaaea4.png 760w,
               /post/peer-review/jp-gone_hu1e39a4b3266917a997281e8904a8c684_51636_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;http://sbh4th.github.io/post/peer-review/jp-gone_hu1e39a4b3266917a997281e8904a8c684_51636_438014fda2823f3009175cb8e99fc031.png&#34;
               width=&#34;760&#34;
               height=&#34;343&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;What the hell kind of clown show is going on here?!&lt;/p&gt;
&lt;p&gt;Finally, it&amp;rsquo;s also important to say that NBER papers are &lt;em&gt;not&lt;/em&gt; peer-reviewed and I&amp;rsquo;m sure Anderson et al.&amp;rsquo;s paper will also benefit from the scrutiny of the peer review process. I just hope that the peer reviewers who use their limited time to provide this crucial activity, &lt;strong&gt;free-of-charge&lt;/strong&gt;, have their reports faithfully considered by editors when making judgements about the paper&amp;rsquo;s merits.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Resource Sharing to Improve Research Quality</title>
      <link>http://sbh4th.github.io/publication/hamra-2019-aa/</link>
      <pubDate>Thu, 01 Aug 2019 00:00:00 +0000</pubDate>
      <guid>http://sbh4th.github.io/publication/hamra-2019-aa/</guid>
      <description></description>
    </item>
    
    <item>
      <title>420 update</title>
      <link>http://sbh4th.github.io/post/420-update/</link>
      <pubDate>Fri, 08 Feb 2019 00:00:00 +0000</pubDate>
      <guid>http://sbh4th.github.io/post/420-update/</guid>
      <description>&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;p&gt;Last fall &lt;a href=&#34;https://twitter.com/AdamPalayew&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Adam Palayew&lt;/a&gt; and I &lt;a href=&#34;http://samharper.org/new-blog/2018/4/17/is-420-deadly&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;wrote&lt;/a&gt; about an interesting &lt;a href=&#34;https://jamanetwork.com/journals/jamainternalmedicine/article-abstract/2672202&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper&lt;/a&gt; that claimed that the number of people in fatal traffic crashes increased by 12% on April the 20th. For those that don&amp;rsquo;t know, April 20th (or &amp;lsquo;420&amp;rsquo;) is a day of celebration of all things cannabis, with large outdoor gatherings where cannabis consumption is promoted, especially at 4:20pm. The idea behind the paper was that excess consumption of cannabis on April 20th led to more drugged driving, and therefore more fatal accidents.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s an interesting and perhaps not implausible idea but, as always, the devil is in the details. The authors, John Staples and Donald Redelmeier used data from the Fatal Accident Accident Reporting System (FARS) and compared drivers involved in fatal accidents on 420 to those involved in accidents one week before and after for the period 1992-2016. This is an interesting design because it inherently compares fatal accidents on the same day of the week, and there is a lot of daily variation in fatal crashes (weekends are particularly deadly).&lt;/p&gt;
&lt;p&gt;They reported that the number of drivers involved in fatal accidents was 12% higher on 420 compared to control days. This paper made quite a splash in the media, and was the 8th most talked about article for &lt;em&gt;JAMA Internal Medicine&lt;/em&gt; in all of 2018:
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://sbh4th.github.io/img/420/jama-1.png&#34; alt=&#34;header&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://sbh4th.github.io/img/420/jama-2.png&#34; alt=&#34;header&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Of course it also was widely discussed on social media and twitter, for example:&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Celebrating &lt;a href=&#34;https://twitter.com/hashtag/420day?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#420day&lt;/a&gt;? Maybe also partake an Uber. &lt;a href=&#34;https://t.co/SCRxSoTAG4&#34;&gt;https://t.co/SCRxSoTAG4&lt;/a&gt; &lt;a href=&#34;https://t.co/BZsiclK63L&#34;&gt;pic.twitter.com/BZsiclK63L&lt;/a&gt;&lt;/p&gt;&amp;mdash; David Juurlink (@DavidJuurlink) &lt;a href=&#34;https://twitter.com/DavidJuurlink/status/987364170847965184?ref_src=twsrc%5Etfw&#34;&gt;April 20, 2018&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;And the authors also made some pretty strong claims in a &lt;a href=&#34;https://www.sciencedaily.com/releases/2018/02/180212112005.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;press release&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;Assuming fewer than 10 per cent of Americans drive while high on April 20, our results suggest that drug use at 420 celebrations more than doubles the risk of a fatal crash.&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;-Donald Redelmeier, February 12, 2018&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is a serious claim, though one might argue that the use of the relative term &amp;ldquo;doubling&amp;rdquo; doesn&amp;rsquo;t help to quantify how serious the problem might be in the absence of base rates. However, at a time of both liberalization of legislation for recreational marijuana and concern about the impacts of drugged driving, such claims naturally prompted other to look more carefully at the evidence.&lt;/p&gt;
&lt;p&gt;Jayson Aydelotte and colleages wrote a &lt;a href=&#34;https://jamanetwork.com/journals/jamainternalmedicine/article-abstract/2712270&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;letter to the editor&lt;/a&gt; at &lt;em&gt;JAMA Internal Medicine&lt;/em&gt; specifically interrogating both the validity of considering two days a week before and after as control conditions, and, since 420 has been gaining in popularity, whether any impact might have increased over time.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Disaggregating the control dates provided additional insights. When comparing the number of crashes for each study date over the study period, in some years the number of crashes on April 20 was quite similar to the number of crashes occurring on at least 1 of the control dates. The April 20 crash rate remained significantly higher than the April 13 crash rate (IRR, 1.12; 95% CI, 1.02-1.23) but was not significantly different from the April 27 crash rate (IRR, 1.09; 95%CI, 0.99-1.20).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I&amp;rsquo;ll leave aside the continued focus on arbitrarily dividing the results into &amp;ldquo;significant&amp;rdquo; and &amp;ldquo;not significant&amp;rdquo;, which is not helpful, but they argue that April 13 and April 27 do not appear to be &amp;lsquo;equivalent&amp;rsquo; control days (let&amp;rsquo;s be honest, however, that the intervals 1.02-1.23 and 0.99-1.20 are in no way substantively different). They also found weaker effects in more recent years, which is again perhaps at odds with what one would expect if 420 has become more popular.&lt;/p&gt;
&lt;h2 id=&#34;update&#34;&gt;Update&lt;/h2&gt;
&lt;p&gt;Adam and I also did our own analysis, which has now been published in the journal &lt;em&gt;Injury Prevention&lt;/em&gt;, and we were able to replicate the findings of Staples and Redelmeier, but we also ran some additional sensitivity analyses and tried to put the findings in context of what we already know about &amp;ldquo;risky&amp;rdquo; driving days.
A few key findings emerged from the paper, and I&amp;rsquo;ll just post a few figures from the paper to try and make the case.&lt;/p&gt;
&lt;h3 id=&#34;control-days-and-uncertainty&#34;&gt;Control days and uncertainty&lt;/h3&gt;
&lt;p&gt;First, there is a somewhat arcane technical issue related to how to calculate uncertainty around the estimated increase in fatal accidents that happen on 420 relative to &amp;ldquo;control&amp;rdquo; days. Sadly, this is relevant because many prominent journals (including leading medical journals) use filters related to &amp;ldquo;statistical significance&amp;rdquo; as a way to highlight &amp;ldquo;novel&amp;rdquo; findings. In practice, that means it&amp;rsquo;s interesting only if &lt;mark&gt;p &amp;lt; 0.05&lt;/mark&gt;,&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; and it is perhaps no surprise that the 420 paper found that, &amp;ldquo;The risk of a fatal crash was &lt;mark&gt;significantly&lt;/mark&gt; higher on April 20 (relative risk, 1.12; 95% CI, 1.05-1.19; P = .001).&amp;rdquo;&lt;/p&gt;
&lt;p&gt;We investigated this by: a) looking at alternative ways of estimating uncertainty; b) adding control days two weeks before/after the index date of 420 or using all other days as the control period; and c) comparing 420 to July 4th (previously shown to demonstrate excess deaths). Here is a figure showing the comparison between 420 and July 4th (left and right panels, respectively), and for different control periods (upper and lower panels, respectively):&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://sbh4th.github.io/img/420/420-fig1.png&#34; alt=&#34;header&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;The thing to notice here is the greater uncertainty that is evident when one takes a more robust approach to estimating the variance, which may have tipped the paper into the &lt;em&gt;p&lt;/em&gt; &amp;gt;0.05 of what Edward Tufte used to &lt;a href=&#34;https://www.edwardtufte.com/bboard/q-and-a-fetch-msg?msg_id=0001et&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;call&lt;/a&gt; the &amp;ldquo;zone of boredom, ambiguity, and unpublishability&amp;rdquo;. But you can also see that expanding the control period to 2 weeks (or the rest of the year) diminshes any impact of 420, but not July 4th, which appears very reliably associated with excess deaths.&lt;/p&gt;
&lt;h3 id=&#34;compared-to-what&#34;&gt;Compared to what?&lt;/h3&gt;
&lt;p&gt;Second, we also tried to look at how the &amp;ldquo;excess risk&amp;rdquo; on 420 compares with similar risks on all other days of the year. We basically just ran the same simple negative binomial regression for each day using 2 control days a week apart for each day of the year. The graph below shows the results on the (log) rate ratio scale:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://sbh4th.github.io/img/420/420-fig2.png&#34; alt=&#34;header&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;You can see from the figure above that the excess risk on 420 does not seem readily distinguishable from daily variations across the entire year. Some clear risky days stand out (July 4th, for example), but if one applies the same filter as the 420 paper and just looks for days when there are &amp;ldquo;significant&amp;rdquo; relative excesses (or deficits) of fatal crashes,&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; there are actually a lot of days that need explaining:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://sbh4th.github.io/img/420/420-t8.png&#34; alt=&#34;header&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;I wonder if one just picked random years whether any of these days would be reliably associated with excess deaths? I would gather July 4th but, as it stands, it is hard to find plausible explanations for why some of these days stand out.&lt;/p&gt;
&lt;h3 id=&#34;transient-or-persistent&#34;&gt;Transient or persistent?&lt;/h3&gt;
&lt;p&gt;Finally, we used a Bayesian hierarchical model and partial pooling across years to evaluate whether any impact of 420 changed over time (this was also partially addressed by the letter from Aydelotte et al.). The plots below show results from models where we allowed the impact of certain &amp;ldquo;key&amp;rdquo; dates well-known to be associated with excess fatal crashes (July 4th, Labor Day weekend) to vary over time. Since 420 is a recent phenomenon we wondered whether we could see changes over time (increases?) in excess deaths. What does seem clear is that there is no reliable excess of drivers involved in fatal crashes on 420, but very clear excesses (and deficits) on other days. This speaks to whether one might just argue that the data are too noisy to detect any reliable signal.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://sbh4th.github.io/img/420/420-fig3.png&#34; alt=&#34;header&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;A couple of final points. As we mentioned in the other &lt;a href=&#34;http://samharper.org/new-blog/2018/4/17/is-420-deadly&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;post&lt;/a&gt;, there are also important questions about whether, practically speaking, it is plausible that drugged driving on 420 would be serious enough to raise the population-wide rate of fatal accidents by something like 12%. It&amp;rsquo;s hard to believe, and our back-of-the-envelope calculations suggested that around 15% of the population would need to be driving while high on 420 from 4:20pm to midnight to generate the kinds of excess rates seen in the Staples/Redelmeier paper.&lt;/p&gt;
&lt;p&gt;And I should also say that this paper was not an attack on the original work by Staples and Redelmeier. I will cop to seeing the paper and being skeptical about the results, but through the process of trying to reproduce their results, we started to think about new questions that could be asked of the data, what kinds of methods might be necessary to answer a question like this, and in general I think this ended up being a really interesting process of discovery. I should also say that because we decided to make all of the data and code available when we submitted the paper (which was first rejected by the &lt;em&gt;Canadian Medical Association Journal&lt;/em&gt;), the review process at &lt;em&gt;Injury Prevention&lt;/em&gt; was really productive:&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;1/7: Thread related to a recent experience with &lt;a href=&#34;https://twitter.com/hashtag/reproducible?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#reproducible&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/research?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#research&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/hashtag/peer?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#peer&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/review?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#review&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/epitwitter?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#epitwitter&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/openscience?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#openscience&lt;/a&gt;&lt;br&gt;&lt;br&gt;I have recently begun uploading data (where feasible) and code to an accessible public repository (&lt;a href=&#34;https://twitter.com/OSFramework?ref_src=twsrc%5Etfw&#34;&gt;@OSFramework&lt;/a&gt;) to reproduce results at the time of submission.&lt;/p&gt;&amp;mdash; Sam Harper (@sbh4th) &lt;a href=&#34;https://twitter.com/sbh4th/status/1070710180361441280?ref_src=twsrc%5Etfw&#34;&gt;December 6, 2018&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;h2 id=&#34;paper--data--code&#34;&gt;Paper + Data + Code&lt;/h2&gt;
&lt;p&gt;If anyone is interested in the paper the gated version is now &lt;a href=&#34;https://injuryprevention.bmj.com/content/early/2019/01/28/injuryprev-2018-043068&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;posted&lt;/a&gt; on &lt;em&gt;Injury Prevention&amp;rsquo;s&lt;/em&gt; website, but we have posted a &lt;a href=&#34;https://osf.io/tzcsy/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;preprint&lt;/a&gt; on my Open Science Foundation page, and we have made all of the &lt;a href=&#34;https://osf.io/jw258/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;raw data&lt;/a&gt; and &lt;a href=&#34;https://osf.io/ck3db/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code&lt;/a&gt; available to reproduce our results publicly available as well. Hope you find it interesting!&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Unless, of course, the paper &lt;em&gt;overturns&lt;/em&gt; a prior result where &lt;em&gt;p&lt;/em&gt; &amp;lt; 0.05, in which case &lt;em&gt;p&lt;/em&gt; &amp;gt; 0.05 is the filter. [See the letter above by Aydelotte et al. for evidence.] Sigh.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;I know what I said earlier (irony), but just trying to make a point here.&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
  </channel>
</rss>
